# 爬虫技术

>  爬虫最早来自一个大学教授，根据网页连接其他网页，最终组成一个网状结构的事实，从种子网页出发，爬取整个互联网。后来为搜索引擎所用，为了让用户能够搜索到互联网上的网页，首先要做的就是爬取这些网页，接着给这些网页建立索引，知道某个关键词在网页什么地方，根据算法调整页面权重，觉得哪个网页排在搜索结果前面。谷歌就是因为Page Rank算法得到比其他搜索引擎更好的结果而打败竞争对手的。

>  爬取一个网站，通常采取深度优先的策略，比如从一个连接深入九层后就不再深入下去，这样既可以防止递归不完，又可以防止中间有两个或多个网页间来回跳转的问题。广度优先通常没有被采用，因为深度优先可以尽快把一个网站爬完。
    像谷歌和百度这样的搜索引擎，有站长工具帮助网站被爬取，又有robots协议，通常只要用通用的爬虫爬取就可以了，而现实中，很多公司为了获取数据源，都要从网上爬取海量的数据，但这些爬取行为本身是目标网站需要禁止的，所以就产生很多攻防战。

>  从最简单的爬虫介绍起。比如我要爬网易首页，每小时爬一次查看最新的新闻，那我要做的就是去<http://www.163.com/> 做http get 请求，得到html格式的网页，通过regular expression，xpath或者css select这样的字符串寻找或者DOM树寻找语法得到相应的文本，做去重并保存入库就结束。

>  复杂一些的爬虫，比如要爬取百度百科动物分类下所有词条。那我首先要分析动物页面<http://baike.baidu.com/fenlei/动物> 下的分类是怎样的。首先动物分类有很多下级分类，下级分类还有下级分类，同时动物分类与其子分类下都有词条可以爬取。由于分类中，大类和子类有递进导致爬取顺序关联，可以先集中爬取所有分类，保存数据库，基于这些分类再分布爬取所有词条。

>  从数量上，可以无限扩充要爬取内容的范围，进而产生一些分布式爬取的思想。比如我是京东，要宣传说我要打价格战，就需要爬取相关行业的一堆网站。首先问题是请求网页众多，于是我要把请求分布到不同网络的机器中，最大化的利用网络资源；第二，抓取类目的时候页面少，速度非常快，而详细页面则数量众多，需要花很长时间来做下载，所以把类目的爬取和详细页面爬取解耦，分两步，快速爬类目并发现新类目，得到内容放入database或者message queue，从DB或MQ读类目并多台机器并发抓取详细内容；第三，多台机器抓取，大家需要协作，保证一台机器知道其他机器是否已经下载了这个页面，把要抓取的页面放入线程安全的分布式Message Queue中，当一台机器需要抓取，就从MQ中取抓取连接，抓取总有失败和超时的，当这台机器没有把抓取完毕或者已知网页错误消息通知MQ，MQ需要把刚才出队的连接再次入队；第四，分布式的Message Queue可以通过consitent hash让自己知道一个网页究竟存储在分布式队列的什么地方，并存取之；第五，网络请求响应很慢，所以基本一台机器在下载网页的时候，其他什么事都没干，是IO bound的事情，但分析网页是CPU bound的事情，当把下载和分析解耦之后，这两个worker 可以跑在一台机器上，最大化压榨机器硬件资源；第六、同一台机器上可以起多个线程进行爬取，python有GIL，可以用Gevent这个异步的Event loop模型来爬取，一台机器IP多线程下载可能被封，可以在网上找一些高匿代理用程序测试代理可用性，得到一个可用代理池分配给每个下载线程。

>  如果你京东要爬我当当数据，我凭什么就随随便便的让你爬呢？

>  我有多种方式能够限制你，我反爬虫。我可以针对IP做请求限制，一分钟请求不同网页过多，我就暂时屏蔽你；一些商品折扣信息需要登录才能看到，登录时候我加验证码，让你难以登录；HTTP GET我家referer限制；我用JS渲染网页内容，你不能仅仅下载网页，你还要解析JS数据和请求；同一页面用不同的几套模版。

>  京东也有办法，登录分析一下当当整体的HTTP POST请求，验证码用神经网络训练机器破解，严格按照referer跳转爬取，分布式爬取或者用代理爬取解除IP限制，用google V8解析JS数据，甚至发现你有些数据内容是通过Restful API请求获得，则直接请求数据而省去解析JS的步骤；同一页面不同模版分情况解析。一些JS，ajax请求交互比较复杂，只能通过模拟浏览器（selenium）这样比较耗费CPU和内存资源的方式来完成。

>  当当可以设置更难识别的验证码，更智能的爬虫行为监测，当然京东也可以……

>  爬虫和反爬虫是一场没有尽头的攻防战，就看你愿意投入多大的成本做这件事，为了防止爬取我可以一周换一次验证码格式，这是反爬虫人力开发成本，为了爬去我愿意每周都去破解，这是爬取开发成本。当反爬虫成本投入越多，愿意投入相应成本进行破解爬取的公司就相应的越少。

>  有不少情况都会涉及到网站的改版，网站不可用，网站自身的错误，这就需要人工的分析，来修改爬虫，也是不少的工作量。需要有一个监控程序来检测对方网页是否还在运行，我们不停的抽取几个页面，来与现在的爬虫结果做对比，测试与验证爬虫的正确性以及当前可用性。
  


### 爬虫下载框架的逻辑思考：

1. 网页下载和网页分析分离。通过message queue解耦。
2. 如果下载有两层，两层是有顺序关系的。再次通过message queue解耦。
3. 如果第一层下载，第一页下载后分析后，知道第一层有三页，则把剩下两页放入message queue。
4. 怎么知道第一层的三页都下载好了，要是一页下载失败了怎么办？要是一页里面条目分析不全或失败怎么办（这个应该算测试的问题）？下载加入状态保存并且持久化，但下载完解析失败就产生数据不一致。
5. 解析中一种规则不对，需要换另一种。所以用配置文件做规则就需要可以解析多个配置文件。
6. 爬虫测试框架

